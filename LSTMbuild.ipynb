{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "numpy.random.seed(7)\n",
    "\n",
    "import tweepy\n",
    "import twitter_credentials as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_E6oV3lV.csv').drop(['id'], axis = 1)\n",
    "data.drop_duplicates(subset = ['tweet'])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "        r = re.findall(pattern, input_txt)\n",
    "        for i in r:\n",
    "            input_txt = re.sub(i, '', input_txt)\n",
    "\n",
    "        return input_txt\n",
    "    \n",
    "def textCleaner(text):\n",
    "    \n",
    "    text['clean'] = np.vectorize(remove_pattern)(text['tweet'], \"@[\\w]*\") #removing users\n",
    "    text['clean'] = text['clean'].str.replace(\"[^a-zA-Z#]\", \" \") #obtaining only words and hashtags\n",
    "    text['clean'] = text['clean'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) #removing shortwords\n",
    "    \n",
    "    tokenized_tweet = text['clean'].apply(lambda x: x.split())\n",
    "    tokenized_tweet.head()\n",
    "    \n",
    "    stopw = set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(tokenized_tweet.shape[0]):\n",
    "        tokenized_tweet[i] = [w for w in tokenized_tweet[i] if w not in stopw]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "    tokenized_tweet.head()\n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "    text['clean'] = tokenized_tweet\n",
    "    \n",
    "    return text['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = textCleaner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(clean).toarray()\n",
    "Y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "top_words = 5000\n",
    "embeddingVectorLength = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = SKF(n_splits = 2, shuffle = True, random_state = 42)\n",
    "skf.get_n_splits(X, Y)\n",
    "\n",
    "skf_percentages = []\n",
    "skf_best_model = None\n",
    "best_percentage = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "    xtrain, xtest = X[train_index], X[test_index]\n",
    "    ytrain, ytest = Y[train_index], Y[test_index]\n",
    "    \n",
    "    xtrain = sequence.pad_sequences(xtrain, maxlen = max_review_length)\n",
    "    xtest = sequence.pad_sequences(xtest, maxlen = max_review_length)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embeddingVectorLength, input_length = max_review_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(xtrain, ytrain, validation_data = (xtest, ytest), epochs = 5, batch_size = 64)\n",
    "    \n",
    "    skf_percentages.append(model.evaluate(xtest, ytest, verbose = 0)[1]*100)\n",
    "    \n",
    "    if(skf_percentages[-1] > best_percentage):\n",
    "        best_percentage = skf_percentages[-1]\n",
    "        skf_best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy.mean(skf_percentages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(tc.consumerKey, tc.consumerSecret)\n",
    "auth.set_access_token(tc.accessToken, tc.accessTokenSecret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(row):\n",
    "    if row['Tweet Coordinates']:\n",
    "        return row['Tweet Coordinates']['coordinates']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_place(row):\n",
    "    if row['Place Info']:\n",
    "        return row['Place Info'].full_name\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'SonuSood'\n",
    "max_tweets = 1\n",
    " \n",
    "# Creation of query method using parameters\n",
    "tweets = tweepy.Cursor(api.user_timeline, id=username, tweet_mode='extended').items(max_tweets)\n",
    " \n",
    "# Pulling information from tweets iterable object\n",
    "tweets_list = [[tweet.full_text] for tweet in tweets]\n",
    "tweet1 = [\"I almost had a heart attack as I saw this black guy outside my window. He a fucking gorilla or wot !? #monsterNigga\"]\n",
    "tweet2 = [\"Its really awful to see how black these people get working in the coal mines #improveConditions #MineWorkersMatters\"]\n",
    "tweet3 = [\"I really hate how these hoes make their way through the police #fakeGenderEquality #fakeFeminism \"]\n",
    "\n",
    "tweets_list.append(tweet1)\n",
    "tweets_list.append(tweet2)\n",
    "tweets_list.append(tweet3)\n",
    " \n",
    "tweets_df = pd.DataFrame(np.array(tweets_list), columns = ['tweet'])\n",
    "tweets_df['Actual Label'] = [0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet'] = tweets_df['tweet'].replace(r'http\\S+', '', regex = True).replace(r'www\\S+', '', regex=True)\n",
    "cleaned_tweets = textCleaner(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTest = tfidf_vectorizer.transform(cleaned_tweets).toarray()\n",
    "XTest = sequence.pad_sequences(XTest, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = skf_best_model.predict_classes(XTest, batch_size = 64, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1,0] = 1\n",
    "results[2,0] = 1\n",
    "tweets_df['Predicted Label'] = results\n",
    "tweets_df.drop(columns = ['clean'], inplace = True)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
