{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "numpy.random.seed(7)\n",
    "\n",
    "import tweepy\n",
    "import twitter_credentials as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_E6oV3lV.csv').drop(['id'], axis = 1)\n",
    "data.drop_duplicates(subset = ['tweet'])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "        r = re.findall(pattern, input_txt)\n",
    "        for i in r:\n",
    "            input_txt = re.sub(i, '', input_txt)\n",
    "\n",
    "        return input_txt\n",
    "    \n",
    "def textCleaner(text):\n",
    "    \n",
    "    text['clean'] = np.vectorize(remove_pattern)(text['tweet'], \"@[\\w]*\") #removing users\n",
    "    text['clean'] = text['clean'].str.replace(\"[^a-zA-Z#]\", \" \") #obtaining only words and hashtags\n",
    "    text['clean'] = text['clean'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) #removing shortwords\n",
    "    \n",
    "    tokenized_tweet = text['clean'].apply(lambda x: x.split())\n",
    "    tokenized_tweet.head()\n",
    "    \n",
    "    stopw = set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(tokenized_tweet.shape[0]):\n",
    "        tokenized_tweet[i] = [w for w in tokenized_tweet[i] if w not in stopw]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "    tokenized_tweet.head()\n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "    text['clean'] = tokenized_tweet\n",
    "    \n",
    "    return text['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = textCleaner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(clean).toarray()\n",
    "Y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "top_words = 5000\n",
    "embeddingVectorLength = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "250/250 [==============================] - 28s 52ms/step - loss: 0.3222 - accuracy: 0.9133 - val_loss: 0.2576 - val_accuracy: 0.9299\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 0.2606 - accuracy: 0.9283 - val_loss: 0.2561 - val_accuracy: 0.9299\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 0.2519 - accuracy: 0.9310 - val_loss: 0.2568 - val_accuracy: 0.9299\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 0.2536 - accuracy: 0.9303 - val_loss: 0.2541 - val_accuracy: 0.9299\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 11s 46ms/step - loss: 0.2559 - accuracy: 0.9293 - val_loss: 0.2542 - val_accuracy: 0.9299\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "250/250 [==============================] - 15s 48ms/step - loss: 0.3204 - accuracy: 0.9055 - val_loss: 0.2566 - val_accuracy: 0.9299\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.2553 - accuracy: 0.9301 - val_loss: 0.2547 - val_accuracy: 0.9299\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.2529 - accuracy: 0.9305 - val_loss: 0.2551 - val_accuracy: 0.9299\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.2566 - accuracy: 0.9291 - val_loss: 0.2540 - val_accuracy: 0.9299\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.2541 - accuracy: 0.9299 - val_loss: 0.2549 - val_accuracy: 0.9299\n"
     ]
    }
   ],
   "source": [
    "skf = SKF(n_splits = 2, shuffle = True, random_state = 42)\n",
    "skf.get_n_splits(X, Y)\n",
    "\n",
    "skf_percentages = []\n",
    "skf_best_model = None\n",
    "best_percentage = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "    xtrain, xtest = X[train_index], X[test_index]\n",
    "    ytrain, ytest = Y[train_index], Y[test_index]\n",
    "    \n",
    "    xtrain = sequence.pad_sequences(xtrain, maxlen = max_review_length)\n",
    "    xtest = sequence.pad_sequences(xtest, maxlen = max_review_length)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embeddingVectorLength, input_length = max_review_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(xtrain, ytrain, validation_data = (xtest, ytest), epochs = 5, batch_size = 64)\n",
    "    \n",
    "    skf_percentages.append(model.evaluate(xtest, ytest, verbose = 0)[1]*100)\n",
    "    \n",
    "    if(skf_percentages[-1] > best_percentage):\n",
    "        best_percentage = skf_percentages[-1]\n",
    "        skf_best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.98542141914368\n"
     ]
    }
   ],
   "source": [
    "print(numpy.mean(skf_percentages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping using Twitter API (Tweepy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(tc.consumerKey, tc.consumerSecret)\n",
    "auth.set_access_token(tc.accessToken, tc.accessTokenSecret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(row):\n",
    "    if row['Tweet Coordinates']:\n",
    "        return row['Tweet Coordinates']['coordinates']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_place(row):\n",
    "    if row['Place Info']:\n",
    "        return row['Place Info'].full_name\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'iamsrk'\n",
    "max_tweets = 1\n",
    " \n",
    "# Creation of query method using parameters\n",
    "tweets = tweepy.Cursor(api.user_timeline, id=username, tweet_mode='extended').items(max_tweets)\n",
    " \n",
    "# Pulling information from tweets iterable object\n",
    "tweets_list = [[tweet.full_text] for tweet in tweets]\n",
    "tweet1 = [\"I almost had a heart attack as I saw this black guy outside my window. He a fucking gorilla or wot !? #monsterNigga\"]\n",
    "tweet2 = [\"Its really awful to see how black these people get working in the coal mines #improveConditions #MineWorkersMatters\"]\n",
    "tweet3 = [\"I really hate how these hoes make their way through the police #fakeGenderEquality #fakeFeminism \"]\n",
    "\n",
    "tweets_list.append(tweet1)\n",
    "tweets_list.append(tweet2)\n",
    "tweets_list.append(tweet3)\n",
    " \n",
    "tweets_df = pd.DataFrame(np.array(tweets_list), columns = ['tweet'])\n",
    "tweets_df['Actual Label'] = [0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet'] = tweets_df['tweet'].replace(r'http\\S+', '', regex = True).replace(r'www\\S+', '', regex=True)\n",
    "cleaned_tweets = textCleaner(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTest = tfidf_vectorizer.transform(cleaned_tweets).toarray()\n",
    "XTest = sequence.pad_sequences(XTest, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ritvik\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "results = skf_best_model.predict_classes(XTest, batch_size = 64, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eid Mubarak to everyone around the world. May ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I almost had a heart attack as I saw this blac...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Its really awful to see how black these people...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I really hate how these hoes make their way th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  Actual Label  \\\n",
       "0  Eid Mubarak to everyone around the world. May ...             0   \n",
       "1  I almost had a heart attack as I saw this blac...             1   \n",
       "2  Its really awful to see how black these people...             0   \n",
       "3  I really hate how these hoes make their way th...             1   \n",
       "\n",
       "   Predicted Label  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['Predicted Label'] = results\n",
    "tweets_df.drop(columns = ['clean'], inplace = True)\n",
    "tweets_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
